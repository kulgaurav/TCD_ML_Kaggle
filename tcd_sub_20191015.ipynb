{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanUp(data):\n",
    "\n",
    "    print(\"\\nBeginning Cleanup...\")\n",
    "    #Imputing data    \n",
    "    data[\"Age\"].fillna(round(data[\"Age\"].mean()),inplace=True)                              #replacing missing values for integer columns\n",
    "    data[\"Year of Record\"].fillna(data[\"Year of Record\"].mode()[0],inplace=True)    \n",
    "    data[\"Size of City\"].fillna(round(data[\"Size of City\"].mean()),inplace=True)    \n",
    "    \n",
    "    data[\"Gender\"].fillna(data[\"Gender\"].mode()[0],inplace=True)                            #replacing missing values for string columns\n",
    "    data[\"Country\"].fillna(data[\"Country\"].mode()[0],inplace=True)\n",
    "    data[\"University Degree\"].fillna(data[\"University Degree\"].mode()[0],inplace=True)\n",
    "    data[\"Hair Color\"].fillna(data[\"Hair Color\"].mode()[0],inplace=True)    \n",
    "    data[['Profession']]=data[['Profession']].fillna(value='9999')                          #replacing profession missing values with 9999\n",
    "    \n",
    "    #data[\"Work Experience in Current Job [years]\"].fillna(data[\"Work Experience in Current Job [years]\"].mean(),inplace=True) \n",
    "    #data['Work Experience in Current Job [years]']=data['Work Experience in Current Job [years]'].astype(str)\n",
    "    data['Housing Situation']=data['Housing Situation'].astype(str)\n",
    "    data['Work Experience in Current Job [years]'].fillna(round(data['Work Experience in Current Job [years]'].mean()),inplace=True)\n",
    "    data['Housing Situation']=data['Housing Situation'].replace('0','zero')\n",
    "    data['Yearly Income in addition to Salary (e.g. Rental Income)'] = data['Yearly Income in addition to Salary (e.g. Rental Income)'].str.replace(r' EUR$', '')\n",
    "    data['Satisfation with employer'].fillna(data['Satisfation with employer'].mode()[0],inplace=True)\n",
    "    data['Yearly Income in addition to Salary (e.g. Rental Income)']=data['Yearly Income in addition to Salary (e.g. Rental Income)'].astype(float)\n",
    "    print(\"\\nCleanup finished...\")    \n",
    "    return data\n",
    "\n",
    "def frequency_time_blocking(data,categoricals,continuous,normalize=True):\n",
    "    for i,cat in enumerate(categoricals):\n",
    "        val_dict = data[cat].value_counts(dropna=False, normalize=normalize).to_dict()\n",
    "        nm = cat + '_FF'\n",
    "        data[nm] = data[cat].map(val_dict).astype('float32')\n",
    "        for j,con in enumerate(continuous):\n",
    "            new_col = cat +'_'+ con\n",
    "            data[new_col] = data[cat].astype(str)+'_'+data[con].astype(str)\n",
    "            temp_df = data[new_col]\n",
    "            fq_encode = temp_df.value_counts(normalize=True).to_dict()\n",
    "            data[new_col] = data[new_col].map(fq_encode)\n",
    "            data[new_col] = data[new_col]/data[cat+'_FF']\n",
    "    return data\n",
    "\n",
    "def removeRows(data):\n",
    "    \n",
    "    print('Removing outliers from Size of City')\n",
    "    outlierCity = detect_outlier(data['Size of City'])                                     \n",
    "    data=data[~data[\"Size of City\"].isin(outlierCity)]\n",
    "    print('Outliers removed from Size of City')\n",
    "    \n",
    "    print('Removing outliers from Total Yearly Income [EUR]')\n",
    "    outlierInc = detect_outlier(data['Total Yearly Income [EUR]'])\n",
    "    data=data[~data[\"Total Yearly Income [EUR]\"].isin(outlierInc)]\n",
    "    print('Outliers removed from Total Yearly Income [EUR]')\n",
    "    \n",
    "    print('Removing outliers from Age')\n",
    "    outlierAge = detect_outlier(data['Age'])\n",
    "    data=data[~data[\"Age\"].isin(outlierAge)]\n",
    "    print('Outliers removed from Age')\n",
    "    \n",
    "    print('Removing outliers from Body Height [cm]')\n",
    "    outlierHt = detect_outlier(data['Body Height [cm]'])\n",
    "    data=data[~data[\"Body Height [cm]\"].isin(outlierHt)]\n",
    "    print('Outliers removed from Body Height [cm]')\n",
    "    \n",
    "    #outlierHS = detect_outlier(data['Housing Situation'])\n",
    "    #data=data[~data[\"Housing Situation\"].isin(outlierHS)]\n",
    "    #outlierSE = detect_outlier(data['Satisfation with employer'])\n",
    "    #data=data[~data[\"Satisfation with employer\"].isin(outlierSE)]\n",
    "    print('Removing outliers from Crime Level in the City of Employement')\n",
    "    outlierCE = detect_outlier(data['Crime Level in the City of Employement'])\n",
    "    data=data[~data[\"Crime Level in the City of Employement\"].isin(outlierCE)]\n",
    "    print('Outliers removed from Crime Level in the City of Employement')\n",
    "    \n",
    "    \n",
    "    #outlierWE = detect_outlier(data['Work Experience in Current Job [years]'])\n",
    "    #data=data[~data[\"Work Experience in Current Job [years]\"].isin(outlierWE)]    \n",
    "    #outlierWE = detect_outlier(data['Yearly Income in addition to Salary (e.g. Rental Income)'])\n",
    "    #data=data[~data[\"Yearly Income in addition to Salary (e.g. Rental Income)\"].isin(outlierWE)]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def detect_outlier(data):\n",
    "    \n",
    "    threshold=3\n",
    "    mean_1 = np.mean(data)\n",
    "    std_1 =np.std(data)\n",
    "    outliers=[]\n",
    "    for y in data:\n",
    "        z_score= (y - mean_1)/std_1 \n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers.append(y)\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"C:\\\\Users\\\\SIDDHARTHA\\\\Dropbox\\\\Trinity Data Science\\\\ML\\\\tcd-ml-comp-201920-income-pred-group\\\\tcd-ml-1920-group-income-train.csv\")\n",
    "\n",
    "test = pd.read_csv(\"C:\\\\Users\\\\SIDDHARTHA\\\\Dropbox\\\\Trinity Data Science\\\\ML\\\\tcd-ml-comp-201920-income-pred-group\\\\tcd-ml-1920-group-income-test.csv\")\n",
    "\n",
    "train.drop_duplicates(inplace=True)\n",
    "test.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1361147, 17)\n",
      "\n",
      "Beginning Cleanup...\n",
      "\n",
      "Cleanup finished...\n",
      "991709\n"
     ]
    }
   ],
   "source": [
    "#print('Train shape orig: ',str(train.shape))\n",
    "#train=removeRows(train)\n",
    "#print('Train shape new: ',str(train.shape))\n",
    "splitter_index=train.shape[0]\n",
    "data = pd.concat([train,test],ignore_index=True)\n",
    "print(data.shape)\n",
    "data = cleanUp(data)  \n",
    "print(splitter_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cats = ['Year of Record', 'Gender', 'Country',\n",
    "        'Profession', 'University Degree','Wears Glasses','Age',\n",
    "        'Hair Color','Housing Situation','Satisfation with employer']\n",
    "#,'Housing Situation','Satisfation with employer']\n",
    "cons = ['Size of City','Body Height [cm]','Crime Level in the City of Employement','Work Experience in Current Job [years]'\n",
    "        ,'Yearly Income in addition to Salary (e.g. Rental Income)']\n",
    "#,'Crime Level in the City of Employement','Work Experience in Current Job [years]','Yearly Income in addition to Salary (e.g. Rental Income)']\n",
    "\n",
    "data = frequency_time_blocking(data,cats,cons)\n",
    "\n",
    "for col in train.dtypes[train.dtypes == 'object'].index.tolist():\n",
    "    feat_le = LabelEncoder()\n",
    "    feat_le.fit(data[col].unique().astype(str))\n",
    "    data[col] = feat_le.transform(data[col].astype(str))\n",
    "\n",
    "del_col = set(['Total Yearly Income [EUR]','Instance'])\n",
    "features_col =  list(set(data) - del_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test = data[features_col].iloc[:splitter_index],data[features_col].iloc[splitter_index:]\n",
    "Y_train = np.log(data['Total Yearly Income [EUR]'].iloc[:splitter_index])\n",
    "X_test_id = data['Instance'].iloc[splitter_index:]\n",
    "x_train,x_val,y_train,y_val = train_test_split(X_train,Y_train,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\ttraining's l1: 0.64849\tvalid_1's l1: 0.649909\n",
      "[2000]\ttraining's l1: 0.35803\tvalid_1's l1: 0.359645\n",
      "[3000]\ttraining's l1: 0.263903\tvalid_1's l1: 0.265326\n",
      "[4000]\ttraining's l1: 0.226654\tvalid_1's l1: 0.228049\n",
      "[5000]\ttraining's l1: 0.206356\tvalid_1's l1: 0.207889\n",
      "[6000]\ttraining's l1: 0.194912\tvalid_1's l1: 0.196511\n",
      "[7000]\ttraining's l1: 0.18794\tvalid_1's l1: 0.189558\n",
      "[8000]\ttraining's l1: 0.182716\tvalid_1's l1: 0.184329\n",
      "[9000]\ttraining's l1: 0.179127\tvalid_1's l1: 0.180738\n",
      "[10000]\ttraining's l1: 0.176169\tvalid_1's l1: 0.177782\n",
      "[11000]\ttraining's l1: 0.173689\tvalid_1's l1: 0.175301\n",
      "[12000]\ttraining's l1: 0.171841\tvalid_1's l1: 0.173461\n",
      "[13000]\ttraining's l1: 0.170306\tvalid_1's l1: 0.171929\n",
      "[14000]\ttraining's l1: 0.169065\tvalid_1's l1: 0.170696\n",
      "[15000]\ttraining's l1: 0.16809\tvalid_1's l1: 0.169728\n",
      "[16000]\ttraining's l1: 0.167257\tvalid_1's l1: 0.168915\n",
      "[17000]\ttraining's l1: 0.166441\tvalid_1's l1: 0.168124\n",
      "[18000]\ttraining's l1: 0.165658\tvalid_1's l1: 0.167382\n",
      "[19000]\ttraining's l1: 0.164866\tvalid_1's l1: 0.16663\n",
      "[20000]\ttraining's l1: 0.164131\tvalid_1's l1: 0.165934\n",
      "[21000]\ttraining's l1: 0.163534\tvalid_1's l1: 0.165378\n",
      "[22000]\ttraining's l1: 0.162884\tvalid_1's l1: 0.164779\n",
      "[23000]\ttraining's l1: 0.162224\tvalid_1's l1: 0.164161\n",
      "[24000]\ttraining's l1: 0.161714\tvalid_1's l1: 0.163696\n",
      "[25000]\ttraining's l1: 0.161189\tvalid_1's l1: 0.163226\n",
      "[26000]\ttraining's l1: 0.160713\tvalid_1's l1: 0.162805\n",
      "[27000]\ttraining's l1: 0.160265\tvalid_1's l1: 0.162415\n",
      "[28000]\ttraining's l1: 0.159807\tvalid_1's l1: 0.162013\n",
      "[29000]\ttraining's l1: 0.159418\tvalid_1's l1: 0.161684\n",
      "[30000]\ttraining's l1: 0.159052\tvalid_1's l1: 0.161374\n",
      "[31000]\ttraining's l1: 0.158715\tvalid_1's l1: 0.161093\n",
      "[32000]\ttraining's l1: 0.15839\tvalid_1's l1: 0.160827\n",
      "[33000]\ttraining's l1: 0.158078\tvalid_1's l1: 0.160577\n",
      "[34000]\ttraining's l1: 0.157764\tvalid_1's l1: 0.16032\n",
      "[35000]\ttraining's l1: 0.157446\tvalid_1's l1: 0.160052\n",
      "[36000]\ttraining's l1: 0.157146\tvalid_1's l1: 0.159801\n",
      "[37000]\ttraining's l1: 0.156832\tvalid_1's l1: 0.159537\n",
      "[38000]\ttraining's l1: 0.15655\tvalid_1's l1: 0.159309\n",
      "[39000]\ttraining's l1: 0.156279\tvalid_1's l1: 0.159101\n",
      "[40000]\ttraining's l1: 0.156035\tvalid_1's l1: 0.158926\n",
      "[41000]\ttraining's l1: 0.155786\tvalid_1's l1: 0.158739\n",
      "[42000]\ttraining's l1: 0.15555\tvalid_1's l1: 0.158565\n",
      "[43000]\ttraining's l1: 0.155313\tvalid_1's l1: 0.15839\n",
      "[44000]\ttraining's l1: 0.155051\tvalid_1's l1: 0.158185\n",
      "[45000]\ttraining's l1: 0.154809\tvalid_1's l1: 0.157999\n",
      "[46000]\ttraining's l1: 0.154559\tvalid_1's l1: 0.157804\n",
      "[47000]\ttraining's l1: 0.154321\tvalid_1's l1: 0.157625\n",
      "[48000]\ttraining's l1: 0.154095\tvalid_1's l1: 0.157458\n",
      "[49000]\ttraining's l1: 0.153868\tvalid_1's l1: 0.157295\n",
      "[50000]\ttraining's l1: 0.153665\tvalid_1's l1: 0.157152\n",
      "[51000]\ttraining's l1: 0.153458\tvalid_1's l1: 0.157001\n",
      "[52000]\ttraining's l1: 0.153256\tvalid_1's l1: 0.156854\n",
      "[53000]\ttraining's l1: 0.153032\tvalid_1's l1: 0.156687\n",
      "[54000]\ttraining's l1: 0.15282\tvalid_1's l1: 0.156535\n",
      "[55000]\ttraining's l1: 0.152637\tvalid_1's l1: 0.156414\n",
      "[56000]\ttraining's l1: 0.15245\tvalid_1's l1: 0.156289\n",
      "[57000]\ttraining's l1: 0.152253\tvalid_1's l1: 0.15616\n",
      "[58000]\ttraining's l1: 0.15204\tvalid_1's l1: 0.156008\n",
      "[59000]\ttraining's l1: 0.151836\tvalid_1's l1: 0.155876\n",
      "[60000]\ttraining's l1: 0.151616\tvalid_1's l1: 0.155724\n",
      "[61000]\ttraining's l1: 0.15144\tvalid_1's l1: 0.155621\n",
      "[62000]\ttraining's l1: 0.151269\tvalid_1's l1: 0.155528\n",
      "[63000]\ttraining's l1: 0.151093\tvalid_1's l1: 0.155423\n",
      "[64000]\ttraining's l1: 0.150912\tvalid_1's l1: 0.155307\n",
      "[65000]\ttraining's l1: 0.15074\tvalid_1's l1: 0.155203\n",
      "[66000]\ttraining's l1: 0.150565\tvalid_1's l1: 0.155095\n",
      "[67000]\ttraining's l1: 0.150394\tvalid_1's l1: 0.154991\n",
      "[68000]\ttraining's l1: 0.150226\tvalid_1's l1: 0.154889\n",
      "[69000]\ttraining's l1: 0.150051\tvalid_1's l1: 0.154772\n",
      "[70000]\ttraining's l1: 0.14989\tvalid_1's l1: 0.154672\n",
      "[71000]\ttraining's l1: 0.149736\tvalid_1's l1: 0.154582\n",
      "[72000]\ttraining's l1: 0.149586\tvalid_1's l1: 0.154493\n",
      "[73000]\ttraining's l1: 0.149436\tvalid_1's l1: 0.154405\n",
      "[74000]\ttraining's l1: 0.149285\tvalid_1's l1: 0.15432\n",
      "[75000]\ttraining's l1: 0.149144\tvalid_1's l1: 0.154242\n",
      "[76000]\ttraining's l1: 0.149003\tvalid_1's l1: 0.154171\n",
      "[77000]\ttraining's l1: 0.148861\tvalid_1's l1: 0.154094\n",
      "[78000]\ttraining's l1: 0.14872\tvalid_1's l1: 0.154011\n",
      "[79000]\ttraining's l1: 0.148568\tvalid_1's l1: 0.153926\n",
      "[80000]\ttraining's l1: 0.148411\tvalid_1's l1: 0.15383\n",
      "[81000]\ttraining's l1: 0.148258\tvalid_1's l1: 0.153737\n",
      "[82000]\ttraining's l1: 0.148104\tvalid_1's l1: 0.153646\n",
      "[83000]\ttraining's l1: 0.147953\tvalid_1's l1: 0.153557\n",
      "[84000]\ttraining's l1: 0.147805\tvalid_1's l1: 0.153467\n",
      "[85000]\ttraining's l1: 0.147662\tvalid_1's l1: 0.153386\n",
      "[86000]\ttraining's l1: 0.147521\tvalid_1's l1: 0.153312\n",
      "[87000]\ttraining's l1: 0.147373\tvalid_1's l1: 0.153228\n",
      "[88000]\ttraining's l1: 0.14721\tvalid_1's l1: 0.153124\n",
      "[89000]\ttraining's l1: 0.147043\tvalid_1's l1: 0.153017\n",
      "[90000]\ttraining's l1: 0.14687\tvalid_1's l1: 0.152901\n",
      "[91000]\ttraining's l1: 0.14671\tvalid_1's l1: 0.152802\n",
      "[92000]\ttraining's l1: 0.146539\tvalid_1's l1: 0.152692\n",
      "[93000]\ttraining's l1: 0.146361\tvalid_1's l1: 0.152575\n",
      "[94000]\ttraining's l1: 0.146227\tvalid_1's l1: 0.152505\n",
      "[95000]\ttraining's l1: 0.146095\tvalid_1's l1: 0.152433\n",
      "[96000]\ttraining's l1: 0.145974\tvalid_1's l1: 0.152375\n",
      "[97000]\ttraining's l1: 0.145862\tvalid_1's l1: 0.152326\n",
      "[98000]\ttraining's l1: 0.145743\tvalid_1's l1: 0.152273\n",
      "[99000]\ttraining's l1: 0.145621\tvalid_1's l1: 0.152216\n",
      "[100000]\ttraining's l1: 0.145498\tvalid_1's l1: 0.152158\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100000]\ttraining's l1: 0.145498\tvalid_1's l1: 0.152158\n"
     ]
    }
   ],
   "source": [
    "iterations=100000\n",
    "stopping_round=500\n",
    "params = {\n",
    "          'max_depth': 20,\n",
    "          'learning_rate': 0.001,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1\n",
    "         }\n",
    "trn_data = lgb.Dataset(x_train, label=y_train)\n",
    "val_data = lgb.Dataset(x_val, label=y_val)\n",
    "# test_data = lgb.Dataset(X_test)\n",
    "clf = lgb.train(params, trn_data, iterations, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=stopping_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10729.218816991232"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "pre_test_lgb = clf.predict(x_val)\n",
    "\n",
    "val_mae = mean_absolute_error(np.exp(y_val),np.exp(pre_test_lgb))\n",
    "val_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_test_lgbdf=pd.DataFrame(pre_test_lgb)\n",
    "pre_test_lgbdf.to_csv('resultval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_test_lgb = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>991709</td>\n",
       "      <td>1</td>\n",
       "      <td>31495.835049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>991710</td>\n",
       "      <td>2</td>\n",
       "      <td>7021.288505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>991711</td>\n",
       "      <td>3</td>\n",
       "      <td>3551.760614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>991712</td>\n",
       "      <td>4</td>\n",
       "      <td>73255.767705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>991713</td>\n",
       "      <td>5</td>\n",
       "      <td>3583.014870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Instance        Income\n",
       "991709         1  31495.835049\n",
       "991710         2   7021.288505\n",
       "991711         3   3551.760614\n",
       "991712         4  73255.767705\n",
       "991713         5   3583.014870"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.DataFrame({'Instance':X_test_id,\n",
    "                       'Income':np.exp(post_test_lgb)})\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.to_csv(\"submission2.csv\",index=False)\n",
    "'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# set width of bar\n",
    "barWidth = 0.25\n",
    " \n",
    "# set height of bar\n",
    "bars1 = [12, 30, 1, 8, 22]\n",
    "bars2 = [28, 6, 16, 5, 10]\n",
    "bars3 = [29, 3, 24, 25, 17]\n",
    " \n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    " \n",
    "# Make the plot\n",
    "plt.bar(r1, bars1, color='#7f6d5f', width=barWidth, edgecolor='white', label='var1')\n",
    "plt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='var2')\n",
    "plt.bar(r3, bars3, color='#2d7f5e', width=barWidth, edgecolor='white', label='var3')\n",
    " \n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('group', fontweight='bold')\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], ['A', 'B', 'C', 'D', 'E'])\n",
    " \n",
    "# Create legend & Show graphic\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
