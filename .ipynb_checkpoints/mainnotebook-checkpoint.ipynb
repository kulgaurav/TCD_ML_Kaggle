{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanUp(data):\n",
    "\n",
    "    print(\"\\nBeginning Cleanup...\")\n",
    "    #Imputing data    \n",
    "    data[\"Age\"].fillna(round(data[\"Age\"].mean()),inplace=True)                              #replacing missing values for integer columns\n",
    "    data[\"Year of Record\"].fillna(data[\"Year of Record\"].mode()[0],inplace=True)    \n",
    "    data[\"Size of City\"].fillna(round(data[\"Size of City\"].mean()),inplace=True)    \n",
    "    \n",
    "    data[\"Gender\"].fillna(data[\"Gender\"].mode()[0],inplace=True)                            #replacing missing values for string columns\n",
    "    data[\"Country\"].fillna(data[\"Country\"].mode()[0],inplace=True)\n",
    "    data[\"University Degree\"].fillna(data[\"University Degree\"].mode()[0],inplace=True)\n",
    "    data[\"Hair Color\"].fillna(data[\"Hair Color\"].mode()[0],inplace=True)    \n",
    "    data[['Profession']]=data[['Profession']].fillna(value='9999')                          #replacing profession missing values with 9999\n",
    "    \n",
    "    #data[\"Work Experience in Current Job [years]\"].fillna(data[\"Work Experience in Current Job [years]\"].mean(),inplace=True) \n",
    "    #data['Work Experience in Current Job [years]']=data['Work Experience in Current Job [years]'].astype(str)\n",
    "    data['Housing Situation']=data['Housing Situation'].astype(str)\n",
    "    data['Work Experience in Current Job [years]'].fillna(round(data['Work Experience in Current Job [years]'].mean()),inplace=True)\n",
    "    data['Housing Situation']=data['Housing Situation'].replace('0','zero')\n",
    "    data['Yearly Income in addition to Salary (e.g. Rental Income)'] = data['Yearly Income in addition to Salary (e.g. Rental Income)'].str.replace(r' EUR$', '')\n",
    "    data['Satisfation with employer'].fillna(data['Satisfation with employer'].mode()[0],inplace=True)\n",
    "    data['Yearly Income in addition to Salary (e.g. Rental Income)']=data['Yearly Income in addition to Salary (e.g. Rental Income)'].astype(float)\n",
    "    print(\"\\nCleanup finished...\")    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataDummies(data):\n",
    "    #one-hot encoding on data\n",
    "    data=pd.get_dummies(data,columns=['Profession','Year of Record','Housing Situation','Gender','Country','University Degree','Wears Glasses','Hair Color','Satisfation with employer'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalizeColTrain(data1,data2):\n",
    "                                                                                            #identifying columns that are different in both data sets\n",
    "    datadiff=data1[data1.columns.difference(data2.columns)]                                 #identifying columns missing in prediction data\n",
    "    for item in datadiff.columns:\n",
    "            data2[item]=0\n",
    "    \n",
    "    datadiff=data2[data2.columns.difference(data1.columns)]                                 #identifying columns missing in training data\n",
    "    for item in datadiff.columns:\n",
    "            data1[item]=0\n",
    "            \n",
    "    data2=data2[data1.columns]                                                              #making sure the series of columns in training and prediction data are same        \n",
    "    \n",
    "    return data1,data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normaliseData(data,feature):\n",
    "       \n",
    "  max_value = data[feature].max()                                                           #using min-max scaling for normalisation  \n",
    "  min_value = data[feature].min()\n",
    "  data[feature] = (data[feature] - min_value) / (max_value - min_value)\n",
    "  return data,max_value,min_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormaliseData(data,feature,max_value,min_value):\n",
    "    data[feature]=data[feature]*(max_value-min_value)+min_value                            #denormalising function\n",
    "    return data\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeRows(data):\n",
    "\n",
    "    #outlierCity = detect_outlier(data['Size of City'])                                     #outlier Identification and removal\n",
    "    #data=data[~data[\"Size of City\"].isin(outlierCity)]\n",
    "    outlierInc = detect_outlier(data['Income in EUR'])\n",
    "    data=data[~data[\"Income in EUR\"].isin(outlierInc)]\n",
    "    #outlierAge = detect_outlier(data['Age'])\n",
    "    #data=data[~data[\"Age\"].isin(outlierAge)]\n",
    "    #outlierHt = detect_outlier(data['Body Height [cm]'])\n",
    "    #data=data[~data[\"Body Height [cm]\"].isin(outlierHt)]\n",
    "    \n",
    "    print(\"\\tRemoving rows with negatives income..\")                                        #removing rows with negative income\n",
    "    data = data[(data['Income in EUR']>=0)]\n",
    "    #print(\"\\tlength: \"+str(len(data)))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outlier(data):\n",
    "    \n",
    "    threshold=3\n",
    "    mean_1 = np.mean(data)\n",
    "    std_1 =np.std(data)\n",
    "    outliers=[]\n",
    "    for y in data:\n",
    "        z_score= (y - mean_1)/std_1 \n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers.append(y)\n",
    "    return outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\SIDDHARTHA\\\\Dropbox\\\\Trinity Data Science\\\\ML\\\\tcd-ml-comp-201920-income-pred-group\\\\tcd-ml-1920-group-income-train.csv\")\n",
    "\n",
    "#dataPred = pd.read_csv(\"C:\\\\Users\\\\SIDDHARTHA\\Dropbox\\\\Trinity Data Science\\\\ML\\\\tcd-ml-comp-201920-income-pred-group\\\\tcd-ml-1920-group-income-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up...\n",
      "\n",
      "Beginning Cleanup...\n",
      "\n",
      "Cleanup finished...\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning up...\")\n",
    "data=cleanUp(data)                                                                      #imputing training data set\n",
    "#dataPred=cleanUp(dataPred)                                                              #imputing prediction data set\n",
    "#print(\"Removing rows...\")\n",
    "#data=removeRows(data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalising...\n",
      "Performing one-hot encoding...\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalising...\")\n",
    "data,maxSizeCityT,minSizeCityT=normaliseData(data,'Size of City')                       #scaling 'Size of City' column in training data set\n",
    "data['Total Yearly Income [EUR]']=np.log(data['Total Yearly Income [EUR]'])                                     #log transformation of Income column\n",
    "#dataPred,maxSizeCityP,minSizeCityP=normaliseData(dataPred,'Size of City')               #scaling 'Size of City' column in prediction data set\n",
    "print(\"Performing one-hot encoding...\")\n",
    "data=getDataDummies(data)                                                               #one-hot encoding on training data\n",
    "#dataPred=getDataDummies(dataPred) \n",
    "print(\"One-hot encoding done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Equalizing columns...\")\n",
    "#data,dataPred=equalizeColTrain(data,dataPred)                                           #equalizing columns in training and prediction data sets\n",
    " \n",
    "#print ('\\tColumns in training :'+str(len(data.columns)))\n",
    "#print ('\\tColumns in prediction :'+str(len(dataPred.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into training and validation data\n",
      "Splitting done..\n",
      "\tTraining: 70.0%\n",
      "\tValidation: 30.0%\n"
     ]
    }
   ],
   "source": [
    "train_X=data[data.columns.difference(['Total Yearly Income [EUR]','Instance'])]            #setting training features\n",
    "train_y=data['Total Yearly Income [EUR]']                                                           #setting up training label\n",
    "#pred_X=dataPred[dataPred.columns.difference(['Income in EUR','Income','Instance'])]     #setting up prediction features\n",
    "    \n",
    "print(\"Splitting into training and validation data\")\n",
    "train_size=0.70\n",
    "X,X_test,y,y_test = train_test_split(train_X,train_y,train_size=train_size,random_state=42)   #splitting training and validation 70-30\n",
    "print('Splitting done..\\n\\tTraining: '+str(train_size*100)+\"%\\n\\tValidation: \"+str(100-(train_size*100))+\"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\ttraining's l2: 0.7648\tvalid_1's l2: 0.765531\n",
      "[2000]\ttraining's l2: 0.411984\tvalid_1's l2: 0.413958\n",
      "[3000]\ttraining's l2: 0.318424\tvalid_1's l2: 0.320752\n",
      "[4000]\ttraining's l2: 0.261286\tvalid_1's l2: 0.263668\n",
      "[5000]\ttraining's l2: 0.218176\tvalid_1's l2: 0.220474\n",
      "[6000]\ttraining's l2: 0.185035\tvalid_1's l2: 0.187256\n",
      "[7000]\ttraining's l2: 0.161711\tvalid_1's l2: 0.163932\n",
      "[8000]\ttraining's l2: 0.145666\tvalid_1's l2: 0.147828\n",
      "[9000]\ttraining's l2: 0.133675\tvalid_1's l2: 0.135785\n",
      "[10000]\ttraining's l2: 0.125113\tvalid_1's l2: 0.127163\n",
      "[11000]\ttraining's l2: 0.118525\tvalid_1's l2: 0.12053\n",
      "[12000]\ttraining's l2: 0.113514\tvalid_1's l2: 0.115474\n",
      "[13000]\ttraining's l2: 0.109424\tvalid_1's l2: 0.111362\n",
      "[14000]\ttraining's l2: 0.105806\tvalid_1's l2: 0.107729\n",
      "[15000]\ttraining's l2: 0.10201\tvalid_1's l2: 0.103913\n",
      "[16000]\ttraining's l2: 0.0978548\tvalid_1's l2: 0.0997527\n",
      "[17000]\ttraining's l2: 0.0947629\tvalid_1's l2: 0.0966612\n",
      "[18000]\ttraining's l2: 0.0923373\tvalid_1's l2: 0.0942394\n",
      "[19000]\ttraining's l2: 0.0900531\tvalid_1's l2: 0.0919605\n",
      "[20000]\ttraining's l2: 0.0879207\tvalid_1's l2: 0.0898362\n",
      "[21000]\ttraining's l2: 0.0857565\tvalid_1's l2: 0.0876701\n",
      "[22000]\ttraining's l2: 0.0835565\tvalid_1's l2: 0.0854681\n",
      "[23000]\ttraining's l2: 0.0813125\tvalid_1's l2: 0.0832266\n",
      "[24000]\ttraining's l2: 0.0792273\tvalid_1's l2: 0.0811449\n",
      "[25000]\ttraining's l2: 0.0772773\tvalid_1's l2: 0.0792004\n",
      "[26000]\ttraining's l2: 0.0752745\tvalid_1's l2: 0.0771975\n",
      "[27000]\ttraining's l2: 0.073513\tvalid_1's l2: 0.0754414\n",
      "[28000]\ttraining's l2: 0.0717097\tvalid_1's l2: 0.0736356\n",
      "[29000]\ttraining's l2: 0.0700106\tvalid_1's l2: 0.0719375\n",
      "[30000]\ttraining's l2: 0.0686727\tvalid_1's l2: 0.0706022\n",
      "[31000]\ttraining's l2: 0.0673197\tvalid_1's l2: 0.0692545\n",
      "[32000]\ttraining's l2: 0.0661571\tvalid_1's l2: 0.0680966\n",
      "[33000]\ttraining's l2: 0.0650909\tvalid_1's l2: 0.0670335\n",
      "[34000]\ttraining's l2: 0.0641926\tvalid_1's l2: 0.0661399\n",
      "[35000]\ttraining's l2: 0.0633371\tvalid_1's l2: 0.065292\n",
      "[36000]\ttraining's l2: 0.0625951\tvalid_1's l2: 0.064561\n",
      "[37000]\ttraining's l2: 0.0618767\tvalid_1's l2: 0.0638537\n",
      "[38000]\ttraining's l2: 0.0612171\tvalid_1's l2: 0.0632026\n",
      "[39000]\ttraining's l2: 0.0605599\tvalid_1's l2: 0.0625572\n",
      "[40000]\ttraining's l2: 0.0599234\tvalid_1's l2: 0.0619319\n",
      "[41000]\ttraining's l2: 0.0593696\tvalid_1's l2: 0.0613915\n",
      "[42000]\ttraining's l2: 0.0588612\tvalid_1's l2: 0.0608965\n",
      "[43000]\ttraining's l2: 0.058338\tvalid_1's l2: 0.0603865\n",
      "[44000]\ttraining's l2: 0.0578396\tvalid_1's l2: 0.0599013\n",
      "[45000]\ttraining's l2: 0.057344\tvalid_1's l2: 0.0594159\n",
      "[46000]\ttraining's l2: 0.0568632\tvalid_1's l2: 0.0589457\n",
      "[47000]\ttraining's l2: 0.0564219\tvalid_1's l2: 0.0585167\n",
      "[48000]\ttraining's l2: 0.0558941\tvalid_1's l2: 0.0579969\n",
      "[49000]\ttraining's l2: 0.055375\tvalid_1's l2: 0.0574867\n",
      "[50000]\ttraining's l2: 0.0549228\tvalid_1's l2: 0.0570434\n",
      "[51000]\ttraining's l2: 0.054543\tvalid_1's l2: 0.0566762\n",
      "[52000]\ttraining's l2: 0.0542034\tvalid_1's l2: 0.0563494\n",
      "[53000]\ttraining's l2: 0.053879\tvalid_1's l2: 0.0560387\n",
      "[54000]\ttraining's l2: 0.0535417\tvalid_1's l2: 0.055714\n",
      "[55000]\ttraining's l2: 0.0532204\tvalid_1's l2: 0.0554068\n",
      "[56000]\ttraining's l2: 0.0528935\tvalid_1's l2: 0.0550948\n",
      "[57000]\ttraining's l2: 0.0525936\tvalid_1's l2: 0.0548098\n",
      "[58000]\ttraining's l2: 0.05231\tvalid_1's l2: 0.0545401\n",
      "[59000]\ttraining's l2: 0.0520333\tvalid_1's l2: 0.0542789\n",
      "[60000]\ttraining's l2: 0.0517674\tvalid_1's l2: 0.0540268\n",
      "[61000]\ttraining's l2: 0.0515107\tvalid_1's l2: 0.0537852\n",
      "[62000]\ttraining's l2: 0.0512505\tvalid_1's l2: 0.0535399\n",
      "[63000]\ttraining's l2: 0.0509788\tvalid_1's l2: 0.0532843\n",
      "[64000]\ttraining's l2: 0.0507309\tvalid_1's l2: 0.0530518\n",
      "[65000]\ttraining's l2: 0.0504696\tvalid_1's l2: 0.052804\n",
      "[66000]\ttraining's l2: 0.0502301\tvalid_1's l2: 0.0525799\n",
      "[67000]\ttraining's l2: 0.0500065\tvalid_1's l2: 0.052371\n",
      "[68000]\ttraining's l2: 0.0497917\tvalid_1's l2: 0.0521716\n",
      "[69000]\ttraining's l2: 0.0495834\tvalid_1's l2: 0.0519784\n",
      "[70000]\ttraining's l2: 0.04938\tvalid_1's l2: 0.0517895\n",
      "[71000]\ttraining's l2: 0.0491863\tvalid_1's l2: 0.0516107\n",
      "[72000]\ttraining's l2: 0.0489949\tvalid_1's l2: 0.0514351\n",
      "[73000]\ttraining's l2: 0.0488006\tvalid_1's l2: 0.0512572\n",
      "[74000]\ttraining's l2: 0.0486081\tvalid_1's l2: 0.0510814\n",
      "[75000]\ttraining's l2: 0.0484286\tvalid_1's l2: 0.0509189\n",
      "[76000]\ttraining's l2: 0.0482529\tvalid_1's l2: 0.0507599\n",
      "[77000]\ttraining's l2: 0.0480756\tvalid_1's l2: 0.0505977\n",
      "[78000]\ttraining's l2: 0.0479046\tvalid_1's l2: 0.0504428\n",
      "[79000]\ttraining's l2: 0.0477464\tvalid_1's l2: 0.0503011\n",
      "[80000]\ttraining's l2: 0.0475939\tvalid_1's l2: 0.050167\n",
      "[81000]\ttraining's l2: 0.0474405\tvalid_1's l2: 0.0500316\n",
      "[82000]\ttraining's l2: 0.0472991\tvalid_1's l2: 0.0499086\n",
      "[83000]\ttraining's l2: 0.0471534\tvalid_1's l2: 0.0497813\n",
      "[84000]\ttraining's l2: 0.0470077\tvalid_1's l2: 0.0496528\n",
      "[85000]\ttraining's l2: 0.046869\tvalid_1's l2: 0.0495314\n",
      "[86000]\ttraining's l2: 0.0467314\tvalid_1's l2: 0.0494108\n",
      "[87000]\ttraining's l2: 0.0465965\tvalid_1's l2: 0.0492919\n",
      "[88000]\ttraining's l2: 0.0464575\tvalid_1's l2: 0.0491685\n",
      "[89000]\ttraining's l2: 0.0463216\tvalid_1's l2: 0.0490485\n",
      "[90000]\ttraining's l2: 0.0461933\tvalid_1's l2: 0.0489361\n",
      "[91000]\ttraining's l2: 0.0460713\tvalid_1's l2: 0.0488304\n",
      "[92000]\ttraining's l2: 0.0459454\tvalid_1's l2: 0.04872\n",
      "[93000]\ttraining's l2: 0.0458306\tvalid_1's l2: 0.0486221\n",
      "[94000]\ttraining's l2: 0.0457169\tvalid_1's l2: 0.0485246\n",
      "[95000]\ttraining's l2: 0.0456033\tvalid_1's l2: 0.0484271\n",
      "[96000]\ttraining's l2: 0.0454931\tvalid_1's l2: 0.0483336\n",
      "[97000]\ttraining's l2: 0.0453839\tvalid_1's l2: 0.0482405\n",
      "[98000]\ttraining's l2: 0.0452788\tvalid_1's l2: 0.0481517\n",
      "[99000]\ttraining's l2: 0.0451714\tvalid_1's l2: 0.0480613\n",
      "[100000]\ttraining's l2: 0.045064\tvalid_1's l2: 0.0479711\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100000]\ttraining's l2: 0.045064\tvalid_1's l2: 0.0479711\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "          'max_depth': 20,\n",
    "          'learning_rate': 0.001,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mse',\n",
    "          \"verbosity\": -1,\n",
    "         }\n",
    "trn_data = lgb.Dataset(X, label=y)\n",
    "val_data = lgb.Dataset(X_test, label=y_test)\n",
    "  \n",
    "clf = lgb.train(params, trn_data, 100000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n",
    "\n",
    "pre_val_lgb = clf.predict(X_test)\n",
    "val_mae = mean_absolute_error(y_test,pre_val_lgb)\n",
    "print(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression:\n",
    "regressor = LinearRegression()                                                          #setting up Linear Regression model\n",
    "regressor.fit(X,y)                                                                      #fitting features and labels\n",
    "y_pred = regressor.predict(X_test)                                                      #predicting on validation features\n",
    "   \n",
    "rms = sqrt(mean_squared_error(np.exp(y_test), np.exp(y_pred)))                          #evaluating RMSE on validation set\n",
    "#rms = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Error: \"+str(rms)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=regressor.predict(pred_X)                                                        #using model to predict Income for prediction feature data\n",
    "#predinc=pd.DataFrame(np.exp(y_pred))                                                   #exponentiating log transformed predictions and storing in data frame\n",
    "#predinc=pd.DataFrame(y_pred)\n",
    "predsubfile = pd.read_csv(args.outputfile)\n",
    "predsubfile['Income']=np.exp(y_pred)\n",
    "predsubfile.to_csv(args.outputfile,index=False)                                                     #exporting predictions to CSVs\n",
    "print(\"Predictions stored at : \"+args.outputfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
